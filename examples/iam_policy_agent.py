#!/usr/bin/env python3
"""
IAM Policy Research Agent with LLM Enhancement

This agent demonstrates the optimal chunking strategy for an IAM policy research system
enhanced with powerful reasoning models (OpenAI o4-mini/o1-mini).

It uses:
1. Fine-grained index (400 tokens) for prompt enhancement and term discovery
2. Contextual index (1200 tokens) for complete examples and policy generation  
3. LLM reasoning for intelligent prompt enhancement and policy creation

By default, when LLM is available, it will:
- Generate complete IAM policies with explanations
- Auto-save policies to generated_policies/ directory
- Save in both JSON and Markdown formats

Usage:
    python examples/iam_policy_agent.py "I need a policy for S3 access"
    python examples/iam_policy_agent.py --enhance-only "database permissions"
    python examples/iam_policy_agent.py --generate-policy "Allow EC2 read access" --context "web application"
    python examples/iam_policy_agent.py "Lambda permissions" --no-save --save-format json
"""

import argparse
import json
import requests
import sys
import os
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from dotenv import load_dotenv
from pathlib import Path

# Load environment variables from .env file
load_dotenv()

# Add src to path for new modules
import sys
from pathlib import Path
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("⚠️  OpenAI not installed. Install with: pip install openai")

# Import new architecture modules
from policy_types import (
    SpecDSL, ReadBack, PolicyArtifacts, IntentExtractionResult,
    spec_dsl_to_json, DSLValidator
)
from intent_extractor import IntentExtractor
from canonizer import Canonizer
from artifact_saver import ArtifactSaver

@dataclass
class PromptEnhancement:
    """Represents an enhancement suggestion for a user prompt."""
    original_prompt: str
    enhanced_prompt: str
    vector_context: List[Dict[str, Any]]  # Raw vector search results
    missing_elements: List[str]
    suggested_specifics: List[str]
    confidence_score: float
    llm_reasoning: Optional[str] = None

@dataclass
class PolicyGenerationContext:
    """Context for generating IAM policies."""
    enhanced_prompt: str
    relevant_examples: List[Dict[str, Any]]
    best_practices: List[str]
    security_considerations: List[str]

@dataclass
class GeneratedPolicy:
    """A complete IAM policy generated by the LLM."""
    policy_json: Dict[str, Any]
    explanation: str
    security_notes: List[str]
    improvement_suggestions: List[str]
    confidence_score: float

class IAMPolicyAgent:
    """IAM Policy Research Agent using dual-index chunking strategy with LLM enhancement."""
    
    def __init__(self, api_base: str = "http://localhost:8000", openai_api_key: Optional[str] = None):
        self.api_base = api_base
        
        # Initialize OpenAI client
        self.openai_client = None
        if OPENAI_AVAILABLE:
            api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
            if api_key:
                try:
                    self.openai_client = openai.OpenAI(api_key=api_key)
                    print("✅ OpenAI client initialized for modern Responses API")
                except Exception as e:
                    print(f"⚠️  OpenAI client initialization failed: {e}")
                    print("   Continuing in vector-only mode...")
                    self.openai_client = None
            else:
                print("⚠️  OpenAI API key not found. Set OPENAI_API_KEY environment variable.")
                print("   Continuing in vector-only mode...")
        
        # Index configurations - using the real IAM indexes that were populated with PDF data
        self.fine_grained_index = {
            "name": "iam-policy-guide-fine", 
            "namespace": "aws-iam-detailed",
            "purpose": "Finding specific terms, concepts, and prompt enhancement"
        }
        self.contextual_index = {
            "name": "iam-policy-guide-context",
            "namespace": "aws-iam-examples", 
            "purpose": "Complete examples, procedures, and policy generation"
        }
        
        # Ensure we're using the direct Pinecone backend where the real indexes exist
        self._ensure_pinecone_backend()
    
    def _ensure_pinecone_backend(self):
        """Ensure we're using direct Pinecone backend where the real indexes exist."""
        try:
            response = requests.post(f"{self.api_base}/config/switch-store?store_type=pinecone")
            if response.status_code == 200:
                print("✅ Using direct Pinecone backend with real IAM indexes")
            else:
                print("⚠️  Warning: Could not switch to Pinecone backend")
        except Exception as e:
            print(f"⚠️  Warning: Backend switch failed: {e}")
    
    def _get_openai_model(self) -> str:
        """Get the best available OpenAI model for prompt enhancement."""
        if not self.openai_client:
            return None
        
        try:
            # For prompt enhancement, use regular models (GPT models are better for instruction following)
            models = self.openai_client.models.list()
            model_names = [model.id for model in models.data]
            
            # Use modern model names from new API
            if "gpt-4.1" in model_names:
                return "gpt-4.1"
            elif "gpt-4o" in model_names:
                return "gpt-4o"
            elif "gpt-4-turbo" in model_names:
                return "gpt-4-turbo"
            else:
                return "gpt-4.1"  # Default to newest model
        except Exception as e:
            print(f"⚠️  Error checking models: {e}")
            return "gpt-4.1"  # Safe fallback to newest model

    def _get_openai_model_for_policy_generation(self) -> str:
        """Get the best available OpenAI model for policy generation (reasoning models preferred)."""
        if not self.openai_client:
            return None
        
        try:
            # For policy generation, reasoning models are excellent for complex analysis
            models = self.openai_client.models.list()
            model_names = [model.id for model in models.data]
            
            # Use modern model names - prefer reasoning models for complex policy generation
            if "o4-mini-2025-04-16" in model_names:
                return "o4-mini-2025-04-16"
            elif "o1-mini" in model_names:
                return "o1-mini"
            elif "gpt-4.1" in model_names:
                return "gpt-4.1"
            elif "gpt-4o" in model_names:
                return "gpt-4o"
            else:
                return "gpt-4.1"  # Default fallback
        except Exception as e:
            print(f"⚠️  Error checking models: {e}")
            return "gpt-4.1"  # Safe fallback
    
    def _search_index(self, query: str, index_config: Dict[str, str], 
                     top_k: int = 5, search_type: str = "semantic") -> List[Dict[str, Any]]:
        """Search a specific index with given parameters."""
        url = f"{self.api_base}/indexes/{index_config['name']}/search/{search_type}"
        payload = {
            "query": query,
            "top_k": top_k,
            "namespace": index_config["namespace"]
        }
        
        try:
            response = requests.post(url, json=payload)
            if response.status_code == 200:
                return response.json().get("results", [])
            else:
                print(f"❌ Search failed: {response.status_code} - {response.text}")
                return []
        except Exception as e:
            print(f"❌ Search error: {e}")
            return []
    
    def _search_with_reranking(self, query: str, index_config: Dict[str, str], 
                              top_k: int = 15, top_n: int = 5) -> List[Dict[str, Any]]:
        """Search with reranking for highest quality results."""
        url = f"{self.api_base}/indexes/{index_config['name']}/search/semantic"
        payload = {
            "query": query,
            "top_k": top_k,
            "namespace": index_config["namespace"],
            "rerank": {
                "model": "pinecone-rerank-v0",
                "topN": top_n,
                "rankFields": ["text"]
            }
        }
        
        try:
            response = requests.post(url, json=payload)
            if response.status_code == 200:
                return response.json().get("results", [])
            else:
                return self._search_index(query, index_config, top_n, "semantic")
        except Exception as e:
            return self._search_index(query, index_config, top_n, "semantic")
    
    def enhance_prompt_with_vector_context(self, user_prompt: str) -> PromptEnhancement:
        """
        Search vector database for IAM documentation and use LLM to enhance prompt.
        
        This is the main enhancement method that:
        1. Searches vector DB for relevant IAM documentation 
        2. Uses LLM with that context to create a detailed, actionable prompt
        """
        # Search for IAM concepts and terminology (focus on fine-grained content)
        concept_results = self._search_with_reranking(
            user_prompt, self.fine_grained_index, top_k=20, top_n=8
        )
        
        # Search for common patterns and actions
        action_query = f"IAM actions permissions {user_prompt}"
        action_results = self._search_index(
            action_query, self.fine_grained_index, top_k=10, search_type="hybrid"
        )
        
        # Combine and deduplicate results
        all_results = concept_results + action_results
        seen_texts = set()
        unique_results = []
        for result in all_results:
            text_snippet = result.get("text", "")[:100]
            if text_snippet not in seen_texts:
                seen_texts.add(text_snippet)
                unique_results.append(result)
        
        # Extract basic enhancement data (fallback if LLM fails)
        missing_elements = []
        suggested_specifics = []
        
        if "resource" not in user_prompt.lower():
            missing_elements.append("Specific AWS resources or ARNs")
        if "action" not in user_prompt.lower() and "permission" not in user_prompt.lower():
            missing_elements.append("Specific AWS actions or permissions")
        if len(user_prompt.split()) < 5:
            missing_elements.append("More detailed context about the use case")
        
        # Basic enhancement (fallback)
        basic_enhanced = user_prompt
        if missing_elements:
            basic_enhanced += f" (Please specify: {', '.join(missing_elements[:3])})"
        
        enhancement = PromptEnhancement(
            original_prompt=user_prompt,
            enhanced_prompt=basic_enhanced,
            vector_context=unique_results[:10],  # Top 10 most relevant results
            missing_elements=missing_elements,
            suggested_specifics=suggested_specifics,
            confidence_score=0.3  # Low confidence until LLM enhancement
        )
        
        # Use LLM to create a much better enhancement with vector context
        if self.openai_client:
            enhancement = self._llm_enhance_with_context(enhancement)
        
        return enhancement
    
    def _llm_enhance_with_context(self, enhancement: PromptEnhancement) -> PromptEnhancement:
        """
        Use LLM with vector search context to create a detailed, actionable prompt.
        """
        model = self._get_openai_model()
        if not model:
            return enhancement
        
        print(f"🧠 Enhancing prompt with {model} using vector search context...")
        
        # Prepare rich context from vector search
        context_text = self._prepare_vector_context_for_llm(enhancement.vector_context)
        
        instructions = """You are an expert AWS IAM policy consultant. You will receive:
1. A user's original request for an IAM policy
2. Relevant AWS IAM documentation found via vector search

Your task is to create an enhanced, detailed prompt that will help generate a precise IAM policy.

The enhanced prompt should include:
- Specific AWS services, actions, and resources
- Security considerations and conditions  
- Clear use case context
- Any missing technical details needed for a production-ready policy

Be specific and actionable. Transform vague requests into detailed policy requirements.

Format your response as:
ENHANCED PROMPT: [your detailed enhanced prompt here]
REASONING: [your analysis of what was missing and how you enhanced it]"""

        user_input = f"""
ORIGINAL USER REQUEST: "{enhancement.original_prompt}"

RELEVANT AWS IAM DOCUMENTATION (from vector search):
{context_text}

Please create an enhanced prompt that includes all the specific technical details needed to generate a production-ready IAM policy. Make it detailed and actionable."""

        try:
            # Use the new Responses API
            response = self.openai_client.responses.create(
                model=model,
                instructions=instructions,
                input=user_input
            )
            
            # Use the new output_text property for simple text access
            llm_response = response.output_text or ""
            
            # Parse the LLM response
            enhanced_prompt, reasoning = self._parse_llm_enhancement_response(llm_response)
            
            # Update enhancement with LLM results
            if enhanced_prompt and enhanced_prompt != enhancement.original_prompt:
                enhancement.enhanced_prompt = enhanced_prompt
                enhancement.llm_reasoning = reasoning
                enhancement.confidence_score = 0.9
                print(f"✅ LLM enhancement successful using {model}")
            else:
                print(f"⚠️  LLM didn't meaningfully enhance prompt, keeping original")
                enhancement.confidence_score = 0.4
            
            return enhancement
            
        except Exception as e:
            print(f"❌ LLM enhancement failed: {e}")
            return enhancement
    
    def _prepare_vector_context_for_llm(self, vector_results: List[Dict[str, Any]]) -> str:
        """Prepare comprehensive vector search context for LLM."""
        context_parts = []
        
        for i, result in enumerate(vector_results[:8], 1):
            text = result.get("text", "")
            metadata = result.get("metadata", {})
            score = result.get("score", 0)
            content_type = metadata.get("content_type", "")
            
            context_parts.append(f"\n--- Result {i} (Score: {score:.3f}, Type: {content_type}) ---")
            context_parts.append(text[:800])  # Much more context than before
        
        return "\n".join(context_parts)
    
    def _parse_llm_enhancement_response(self, llm_response: str) -> Tuple[str, str]:
        """Parse LLM response to extract enhanced prompt and reasoning."""
        enhanced_prompt = ""
        reasoning = ""
        
        lines = llm_response.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Detect sections
            if line.upper().startswith("ENHANCED PROMPT:"):
                current_section = "enhanced"
                prompt_part = line.split(":", 1)[1].strip() if ":" in line else ""
                if prompt_part:
                    enhanced_prompt = prompt_part
                continue
            elif line.upper().startswith("REASONING:"):
                current_section = "reasoning"
                reasoning_part = line.split(":", 1)[1].strip() if ":" in line else ""
                if reasoning_part:
                    reasoning = reasoning_part
                continue
            
            # Add content to current section
            if current_section == "enhanced" and line:
                if enhanced_prompt:
                    enhanced_prompt += " " + line
                else:
                    enhanced_prompt = line
            elif current_section == "reasoning" and line:
                if reasoning:
                    reasoning += " " + line
                else:
                    reasoning = line
        
        return enhanced_prompt.strip(), reasoning.strip()
    
    def generate_policy_context(self, enhanced_prompt: str) -> PolicyGenerationContext:
        """
        Use contextual index to find complete examples and procedures for policy generation.
        
        This searches for larger context chunks that contain complete policy examples,
        step-by-step procedures, and comprehensive guidance.
        """
        print("🎯 Searching contextual index for complete examples...")
        
        # Search for complete policy examples (focus on contextual content)
        examples_results = self._search_with_reranking(
            f"policy example {enhanced_prompt}", self.contextual_index, top_k=20, top_n=6
        )
        
        # Search for best practices
        practices_query = f"IAM best practices security {enhanced_prompt}"
        practices_results = self._search_index(
            practices_query, self.contextual_index, top_k=10, search_type="hybrid"
        )
        
        # Search for security considerations
        security_query = f"security considerations least privilege {enhanced_prompt}"
        security_results = self._search_index(
            security_query, self.contextual_index, top_k=8
        )

        # Extract relevant examples (prioritize contextual and complete_example content)
        relevant_examples = []
        for result in examples_results:
            metadata = result.get("metadata", {})
            text = result.get("text", "")  # Text is at top level
            content_type = metadata.get("content_type", "")  # content_type not chunk_type
            
            # Prioritize contextual chunks and complete examples
            if (content_type in ["policy_example", "procedure", "tutorial"] or 
                any(term in text.lower() for term in ["{", "}", "version", "statement"])):
                relevant_examples.append({
                    "text": text,
                    "score": result.get("score", 0),
                    "service": metadata.get("service", "unknown"),
                    "content_type": content_type  # Use content_type not policy_type
                })
        
        # Extract best practices (prioritize best_practices category)
        best_practices = []
        for result in practices_results[:4]:
            metadata = result.get("metadata", {})
            text = result.get("text", "")  # Text is at top level
            content_type = metadata.get("content_type", "")  # content_type not category
            
            if (content_type == "best_practice" or 
                any(term in text.lower() for term in ["practice", "recommend", "should", "principle"])):
                best_practices.append(text[:300])
        
        # Extract security considerations
        security_considerations = []
        for result in security_results[:3]:
            metadata = result.get("metadata", {})
            text = result.get("text", "")  # Text is at top level
            content_type = metadata.get("content_type", "")
            
            if (content_type in ["security_guideline", "best_practice"] or
                any(term in text.lower() for term in ["security", "least privilege", "deny", "risk"])):
                security_considerations.append(text[:300])
        
        return PolicyGenerationContext(
            enhanced_prompt=enhanced_prompt,
            relevant_examples=relevant_examples,
            best_practices=best_practices,
            security_considerations=security_considerations
        )
    
    def generate_iam_policy(
        self, 
        enhancement: PromptEnhancement, 
        context: PolicyGenerationContext
    ) -> GeneratedPolicy:
        """
        Generate a complete IAM policy using LLM reasoning with full vector search context.
        """
        if not self.openai_client:
            print("⚠️  OpenAI not available, cannot generate policy")
            return None
        
        model = self._get_openai_model_for_policy_generation()  # Use reasoning models for policy generation
        if not model:
            return None
        
        print(f"🏗️  Generating IAM policy with {model}...")
        
        # Prepare comprehensive context for policy generation
        policy_context = self._prepare_comprehensive_policy_context(enhancement, context)
        
        instructions = """You are an expert AWS IAM policy architect. Generate a complete, production-ready IAM policy.

CRITICAL: Your response MUST include a valid JSON policy. Format your response exactly like this:

POLICY:
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [...],
      "Resource": [...]
    }
  ]
}

EXPLANATION: [explain what the policy does]

SECURITY NOTES: [list security considerations]

IMPROVEMENTS: [suggest potential improvements]

The policy must be syntactically correct JSON and follow AWS IAM best practices."""

        user_input = f"""
REQUIREMENT: {enhancement.enhanced_prompt}

AWS DOCUMENTATION CONTEXT:
{policy_context}

Generate a complete IAM policy that fulfills this requirement securely and follows least privilege principles."""

        try:
            # Use the new Responses API
            response = self.openai_client.responses.create(
                model=model,
                instructions=instructions,
                input=user_input
            )
            
            # Use the new output_text property
            llm_response = response.output_text or ""
            
            # Parse the response to extract policy and metadata
            policy_json, explanation, security_notes, improvements = self._parse_policy_response(llm_response)
            
            # Calculate confidence score based on policy completeness
            confidence = self._calculate_policy_confidence(policy_json, explanation)
            
            if policy_json:
                generated_policy = GeneratedPolicy(
                    policy_json=policy_json,
                    explanation=explanation,
                    security_notes=security_notes,
                    improvement_suggestions=improvements,
                    confidence_score=confidence
                )
                
                print(f"✅ IAM policy generated successfully using {model}")
                return generated_policy
            else:
                print(f"❌ No valid policy JSON found in response")
                return None
            
        except Exception as e:
            print(f"❌ Policy generation failed: {e}")
            return None
    
    def _prepare_comprehensive_policy_context(self, enhancement: PromptEnhancement, context: PolicyGenerationContext) -> str:
        """Prepare comprehensive context for policy generation."""
        context_parts = []
        
        # Include the vector search reasoning
        if enhancement.llm_reasoning:
            context_parts.append(f"ENHANCEMENT ANALYSIS:\n{enhancement.llm_reasoning}")
        
        # Include policy examples with more detail
        if context.relevant_examples:
            context_parts.append("\nPOLICY EXAMPLES:")
            for i, example in enumerate(context.relevant_examples[:4], 1):
                context_parts.append(f"\nExample {i}:")
                context_parts.append(example['text'][:600])
        
        # Include best practices
        if context.best_practices:
            context_parts.append("\nBEST PRACTICES:")
            for i, practice in enumerate(context.best_practices[:3], 1):
                context_parts.append(f"{i}. {practice}")
        
        # Include security considerations
        if context.security_considerations:
            context_parts.append("\nSECURITY CONSIDERATIONS:")
            for i, consideration in enumerate(context.security_considerations[:3], 1):
                context_parts.append(f"{i}. {consideration}")
        
        return "\n".join(context_parts)
    
    def _parse_policy_response(self, llm_response: str) -> Tuple[Dict[str, Any], str, List[str], List[str]]:
        """Parse LLM response to extract policy JSON and metadata with improved parsing."""
        policy_json = {}
        explanation = ""
        security_notes = []
        improvements = []
        
        lines = llm_response.split('\n')
        current_section = None
        json_content = ""
        collecting_json = False
        
        for line in lines:
            line_stripped = line.strip()
            line_upper = line_stripped.upper()
            
            # Detect sections
            if line_upper.startswith("POLICY:"):
                current_section = "policy"
                # Check if JSON starts on same line
                if "{" in line:
                    collecting_json = True
                    json_content = line.split(":", 1)[1].strip() if ":" in line else line
                continue
            elif line_upper.startswith("EXPLANATION:"):
                current_section = "explanation"
                explanation = line.split(":", 1)[1].strip() if ":" in line else ""
                continue
            elif line_upper.startswith("SECURITY"):
                current_section = "security"
                continue
            elif line_upper.startswith("IMPROVEMENTS:"):
                current_section = "improvements"
                continue
            
            # Handle JSON collection
            if current_section == "policy" or collecting_json:
                if "{" in line_stripped or collecting_json:
                    collecting_json = True
                    if json_content:
                        json_content += "\n" + line
                    else:
                        json_content = line
                    
                    # Check if JSON is complete
                    if "}" in line_stripped and json_content.count("{") == json_content.count("}"):
                        try:
                            # Clean up the JSON content
                            json_str = json_content.strip()
                            if json_str.startswith("POLICY:"):
                                json_str = json_str.split(":", 1)[1].strip()
                            policy_json = json.loads(json_str)
                            collecting_json = False
                            current_section = None
                        except json.JSONDecodeError as e:
                            print(f"⚠️  JSON parsing error: {e}")
                            continue
                continue
            
            # Handle other sections
            if current_section == "explanation" and line_stripped:
                if explanation:
                    explanation += " " + line_stripped
                else:
                    explanation = line_stripped
            elif current_section == "security" and line_stripped:
                if line_stripped.startswith(("•", "-", "1.", "2.", "3.")):
                    security_notes.append(line_stripped)
                elif line_stripped and not line_upper.startswith(("SECURITY", "NOTES")):
                    security_notes.append(line_stripped)
            elif current_section == "improvements" and line_stripped:
                if line_stripped.startswith(("•", "-", "1.", "2.", "3.")):
                    improvements.append(line_stripped)
                elif line_stripped and not line_upper.startswith("IMPROVEMENTS"):
                    improvements.append(line_stripped)
        
        return policy_json, explanation, security_notes, improvements
    
    def _calculate_policy_confidence(self, policy_json: Dict[str, Any], explanation: str) -> float:
        """Calculate confidence score for generated policy."""
        confidence = 0.5  # Base confidence
        
        # Check policy structure
        if policy_json:
            if "Version" in policy_json:
                confidence += 0.1
            if "Statement" in policy_json and isinstance(policy_json["Statement"], list):
                confidence += 0.1
                # Check statement quality
                for stmt in policy_json["Statement"]:
                    if "Effect" in stmt and "Action" in stmt:
                        confidence += 0.05
                    if "Resource" in stmt:
                        confidence += 0.05
        
        # Check explanation quality
        if explanation and len(explanation.split()) > 20:
            confidence += 0.2
        
        return min(confidence, 1.0)
    
    def _prompt_for_missing_details(self, generated_policy: GeneratedPolicy, enhancement: PromptEnhancement) -> GeneratedPolicy:
        """
        Check if the generated policy has placeholders and prompt user for actual values.
        """
        if not generated_policy or not generated_policy.policy_json:
            return generated_policy
        
        policy_str = json.dumps(generated_policy.policy_json, indent=2)
        placeholders_found = []
        
        # Check for common placeholders
        if "<account-id>" in policy_str or "<account>" in policy_str:
            placeholders_found.append("AWS Account ID")
        if "SecurityRole" in policy_str and "arn:aws:iam::" in policy_str:
            placeholders_found.append("IAM Role Name")  
        if "confidential" in policy_str and "arn:aws:s3:::confidential" in policy_str:
            placeholders_found.append("S3 Bucket Name")
        if "my-app-bucket" in policy_str:
            placeholders_found.append("S3 Bucket Name")
        if "vpce-" in policy_str or "vpc-" in policy_str:
            placeholders_found.append("VPC/VPC Endpoint ID")
        if "kms:.*:key/" in policy_str:
            placeholders_found.append("KMS Key ARN")
        
        if not placeholders_found:
            return generated_policy
        
        print(f"\n🔧 MISSING REQUIRED DETAILS")
        print(f"=" * 50)
        print(f"The generated policy contains placeholders that need real AWS values:")
        for placeholder in placeholders_found:
            print(f"  • {placeholder}")
        
        # Ask user if they want to provide the details now
        try:
            response = input(f"\nWould you like to provide these details now? (y/n): ").strip().lower()
            if response not in ['y', 'yes']:
                print(f"📋 Policy saved with placeholders - remember to update before deployment!")
                return generated_policy
            
            # Collect the actual values
            replacements = {}
            
            if "AWS Account ID" in placeholders_found:
                account_id = input(f"Enter your AWS Account ID (12 digits): ").strip()
                if account_id and len(account_id) == 12 and account_id.isdigit():
                    replacements["<account-id>"] = account_id
                    replacements["<account>"] = account_id
                else:
                    print(f"⚠️  Invalid account ID format, keeping placeholder")
            
            if "IAM Role Name" in placeholders_found:
                role_name = input(f"Enter the exact IAM role name (e.g., MySecurityRole): ").strip()
                if role_name:
                    replacements["SecurityRole"] = role_name
            
            if "S3 Bucket Name" in placeholders_found:
                bucket_name = input(f"Enter the S3 bucket name: ").strip()
                if bucket_name:
                    replacements["confidential"] = bucket_name
                    replacements["my-app-bucket"] = bucket_name
            
            if "VPC/VPC Endpoint ID" in placeholders_found:
                vpc_id = input(f"Enter VPC Endpoint ID (or 'remove' to delete condition): ").strip()
                if vpc_id.lower() == 'remove':
                    print(f"📝 Note: VPC endpoint condition should be manually removed from policy")
                elif vpc_id.startswith('vpce-'):
                    # This would need more complex JSON manipulation
                    print(f"📝 Note: Update VPC endpoint ID manually: {vpc_id}")
            
            # Apply replacements to policy JSON
            updated_policy_str = json.dumps(generated_policy.policy_json, indent=2)
            for placeholder, real_value in replacements.items():
                updated_policy_str = updated_policy_str.replace(placeholder, real_value)
            
            try:
                updated_policy_json = json.loads(updated_policy_str)
                generated_policy.policy_json = updated_policy_json
                generated_policy.explanation += f"\n\nUpdated with user-provided values: {list(replacements.keys())}"
                print(f"✅ Policy updated with your provided details!")
                return generated_policy
            except json.JSONDecodeError:
                print(f"⚠️  Error updating policy JSON, keeping original with placeholders")
                return generated_policy
                
        except KeyboardInterrupt:
            print(f"\n📋 Keeping policy with placeholders")
            return generated_policy
        except Exception as e:
            print(f"⚠️  Error collecting details: {e}")
            return generated_policy

    def research_policy_request(self, user_prompt: str) -> Tuple[PromptEnhancement, PolicyGenerationContext]:
        """
        Complete research pipeline: enhance prompt and gather policy context.
        """
        print("🤖 IAM Policy Research Agent - Dual-Index Analysis")
        print("=" * 60)
        
        # Step 1: Enhance prompt using vector search + LLM
        print("📋 Searching fine-grained index for specific concepts...")
        enhancement = self.enhance_prompt_with_vector_context(user_prompt)
        
        # Step 2: Generate policy context using contextual index  
        context = self.generate_policy_context(enhancement.enhanced_prompt)
        
        print(f"\n✅ Research Complete:")
        print(f"   Enhancement Confidence: {enhancement.confidence_score:.1%}")
        print(f"   Found {len(context.relevant_examples)} policy examples")
        print(f"   Found {len(context.best_practices)} best practices")
        print(f"   Found {len(context.security_considerations)} security notes")
        
        return enhancement, context

    def complete_policy_generation(self, user_prompt: str) -> Tuple[PromptEnhancement, PolicyGenerationContext, GeneratedPolicy]:
        """
        Complete end-to-end policy generation pipeline.
        
        This runs the full workflow: vector search -> LLM prompt enhancement -> policy generation.
        """
        print("🚀 Complete IAM Policy Generation Pipeline")
        print("=" * 60)
        
        # Step 1-3: Research and enhancement
        enhancement, context = self.research_policy_request(user_prompt)
        
        # Step 4: Generate actual IAM policy
        generated_policy = None
        if self.openai_client:
            generated_policy = self.generate_iam_policy(enhancement, context)
            
            # Step 5: Check for missing details and prompt user if needed
            if generated_policy:
                generated_policy = self._prompt_for_missing_details(generated_policy, enhancement)
        else:
            print("⚠️  OpenAI not available - skipping policy generation")
        
        return enhancement, context, generated_policy
    
    def generate_policy_artifacts(self, user_prompt: str) -> Tuple[PolicyArtifacts, List[Dict[str, Any]]]:
        """
        NEW: Generate all four policy artifacts using the updated architecture.
        
        Returns:
        - ReadBack: Human-readable summary
        - SpecDSL: Machine-readable intent with provenance
        - Baseline Policy: Deterministic IAM policy from canonizer
        - Candidate Policy: LLM-generated policy
        """
        print("🚀 NEW: Four-Artifact Policy Generation Pipeline")
        print("=" * 60)
        
        # Step 1: Search vector databases for context
        print("📋 Gathering AWS documentation context...")
        concept_results = self._search_with_reranking(
            user_prompt, self.fine_grained_index, top_k=20, top_n=8
        )
        action_query = f"IAM actions permissions {user_prompt}"
        action_results = self._search_index(
            action_query, self.fine_grained_index, top_k=10, search_type="hybrid"
        )
        
        # Combine and deduplicate RAG context
        all_results = concept_results + action_results
        seen_texts = set()
        unique_results = []
        for result in all_results:
            text_snippet = result.get("text", "")[:100]
            if text_snippet not in seen_texts:
                seen_texts.add(text_snippet)
                unique_results.append(result)
        
        rag_context = unique_results[:15]  # Top 15 most relevant
        
        # Step 2: Extract structured intent (ReadBack + SpecDSL)
        print("🔍 Extracting structured intent with evidence...")
        intent_extractor = IntentExtractor(self.openai_client)
        intent_result = intent_extractor.extract_intent(user_prompt, rag_context)
        
        print(f"✅ Intent extraction complete (confidence: {intent_result.confidence_score:.1%})")
        
        # Step 3: Generate baseline policy from SpecDSL
        print("⚙️  Generating baseline policy from intent specification...")
        try:
            # Validate SpecDSL first
            validation_errors = DSLValidator.validate(intent_result.spec_dsl)
            if validation_errors:
                print(f"⚠️  SpecDSL validation warnings: {'; '.join(validation_errors[:3])}")
            
            baseline_policy = Canonizer.canonize(intent_result.spec_dsl)
            print("✅ Baseline policy generated successfully")
        except Exception as e:
            print(f"❌ Baseline policy generation failed: {e}")
            baseline_policy = {"Version": "2012-10-17", "Statement": []}
        
        # Step 4: Generate candidate policy using existing LLM generator
        print("🧠 Generating candidate policy with LLM...")
        candidate_policy = None
        generation_confidence = 0.0
        
        if self.openai_client:
            # Use existing method but extract just the policy JSON
            enhancement, context = self.research_policy_request(user_prompt)
            generated_policy = self.generate_iam_policy(enhancement, context)
            
            if generated_policy:
                candidate_policy = generated_policy.policy_json
                generation_confidence = generated_policy.confidence_score
                print(f"✅ Candidate policy generated (confidence: {generation_confidence:.1%})")
            else:
                print("❌ Candidate policy generation failed")
                candidate_policy = {"Version": "2012-10-17", "Statement": []}
        else:
            print("⚠️  OpenAI not available - using baseline as candidate")
            candidate_policy = baseline_policy
            generation_confidence = intent_result.confidence_score
        
        # Step 5: Package all artifacts
        artifacts = PolicyArtifacts(
            read_back=intent_result.read_back,
            spec_dsl=intent_result.spec_dsl,
            baseline_policy=baseline_policy,
            candidate_policy=candidate_policy,
            extraction_confidence=intent_result.confidence_score,
            generation_confidence=generation_confidence
        )
        
        print(f"\n✅ All artifacts generated successfully!")
        print(f"   Intent extraction: {intent_result.confidence_score:.1%} confidence")
        print(f"   Policy generation: {generation_confidence:.1%} confidence")
        
        return artifacts, rag_context
    
    def display_results(self, enhancement: PromptEnhancement, context: PolicyGenerationContext):
        """Display comprehensive results for the user."""
        print("\n" + "=" * 60)
        print("📋 DETAILED ANALYSIS RESULTS")
        print("=" * 60)
        
        # Show the enhancement process
        print(f"\n🔍 PROMPT TRANSFORMATION")
        print(f"Enhanced Specification:")
        # Show the full enhanced prompt without truncation
        print(f"{enhancement.enhanced_prompt}")
        
        if enhancement.llm_reasoning:
            print(f"\n🧠 AI Analysis:")
            # Show the full reasoning without truncation
            print(f"{enhancement.llm_reasoning}")
        
        # Policy generation context
        print(f"\n📚 AWS DOCUMENTATION CONTEXT")
        
        if context.relevant_examples:
            print(f"\n📄 Relevant Policy Examples ({len(context.relevant_examples)} found):")
            for i, example in enumerate(context.relevant_examples[:3], 1):
                print(f"\n   Example {i} (Relevance: {example['score']:.3f}):")
                print(f"   {example['text'][:350]}...")
        
        if context.best_practices:
            print(f"\n⭐ Best Practices:")
            for i, practice in enumerate(context.best_practices[:3], 1):
                print(f"\n   {i}. {practice}")
        
        if context.security_considerations:
            print(f"\n🔒 Security Considerations:")
            for i, consideration in enumerate(context.security_considerations[:2], 1):
                print(f"\n   {i}. {consideration}")
    
    def display_generated_policy(self, generated_policy: GeneratedPolicy):
        """Display the complete generated IAM policy with analysis."""
        if not generated_policy:
            print("\n❌ No policy was generated")
            return
        
        print("\n" + "=" * 60)
        print("🏗️  GENERATED IAM POLICY")
        print("=" * 60)
        
        print(f"\n📊 Policy Confidence: {generated_policy.confidence_score:.1%}")
        
        if generated_policy.policy_json:
            print(f"\n📋 IAM Policy JSON:")
            print("```json")
            print(json.dumps(generated_policy.policy_json, indent=2))
            print("```")
        
        if generated_policy.explanation:
            print(f"\n📝 Policy Explanation:")
            print(f"{generated_policy.explanation}")
        
        if generated_policy.security_notes:
            print(f"\n🔒 Security Considerations:")
            for i, note in enumerate(generated_policy.security_notes, 1):
                print(f"   {i}. {note}")
        
        if generated_policy.improvement_suggestions:
            print(f"\n💡 Improvement Suggestions:")
            for i, suggestion in enumerate(generated_policy.improvement_suggestions, 1):
                print(f"   {i}. {suggestion}")
        
        print(f"\n✅ Policy Ready for Review and Testing")
    
    def display_policy_artifacts(self, artifacts: PolicyArtifacts):
        """Display all four policy artifacts in a structured format."""
        print("\n" + "=" * 80)
        print("📋 FOUR-ARTIFACT POLICY ANALYSIS")
        print("=" * 80)
        
        # 1. Read-Back (Human Summary)
        print("\n🔍 1. READ-BACK (Human Summary)")
        print("-" * 40)
        print(f"📝 Summary: {artifacts.read_back.summary}")
        
        if artifacts.read_back.bullets:
            print(f"\n📌 Key Points:")
            for bullet in artifacts.read_back.bullets:
                print(f"  • {bullet}")
        
        if artifacts.read_back.assumptions:
            print(f"\n⚠️  Assumptions:")
            for assumption in artifacts.read_back.assumptions:
                print(f"  • {assumption}")
        
        if artifacts.read_back.risk_callouts:
            print(f"\n🚨 Risk Callouts:")
            for risk in artifacts.read_back.risk_callouts:
                print(f"  • {risk}")
        
        print(f"\n📊 Extraction Confidence: {artifacts.extraction_confidence:.1%}")
        
        # 2. Spec DSL (Machine Intent)
        print(f"\n🏗️  2. SPEC DSL (Machine Intent with Evidence)")
        print("-" * 50)
        print(f"📋 Version: {artifacts.spec_dsl.version}")
        print(f"👤 Principal: {artifacts.spec_dsl.who.get('principal_ref', 'Not specified')}")
        print(f"🌐 Scope: {len(artifacts.spec_dsl.scope.get('accounts', []))} accounts, {len(artifacts.spec_dsl.scope.get('regions', []))} regions")
        
        print(f"\n💡 Capabilities ({len(artifacts.spec_dsl.capabilities)}):")
        for i, cap in enumerate(artifacts.spec_dsl.capabilities, 1):
            mode_info = f" ({cap.mode})" if cap.mode else ""
            print(f"  {i}. {cap.name} - {cap.service}{mode_info}")
            print(f"     Resources: {len(cap.resources)} ARNs")
            if cap.conditions:
                print(f"     Conditions: {len(cap.conditions)} rules")
            print(f"     Evidence: {len(cap.evidence)} citations (avg confidence: {sum(e.confidence for e in cap.evidence) / len(cap.evidence):.0f}%)" if cap.evidence else "     Evidence: No citations")
        
        if artifacts.spec_dsl.must_never:
            print(f"\n🚫 Restrictions ({len(artifacts.spec_dsl.must_never)}):")
            for restriction in artifacts.spec_dsl.must_never:
                print(f"  • {restriction.name}: {restriction.rationale}")
        
        # 3. Baseline Policy (Deterministic)
        print(f"\n⚙️  3. BASELINE POLICY (Deterministic from DSL)")
        print("-" * 50)
        baseline_statements = artifacts.baseline_policy.get("Statement", [])
        print(f"📄 IAM Policy with {len(baseline_statements)} statements")
        
        for i, stmt in enumerate(baseline_statements, 1):
            effect = stmt.get("Effect", "Unknown")
            actions = stmt.get("Action", [])
            if isinstance(actions, str):
                actions = [actions]
            print(f"  {i}. {stmt.get('Sid', f'Statement{i}')} ({effect})")
            print(f"     Actions: {len(actions)} ({', '.join(actions[:3])}{'...' if len(actions) > 3 else ''})")
            resources = stmt.get("Resource", [])
            if isinstance(resources, str):
                resources = [resources]
            print(f"     Resources: {len(resources)} ARNs")
            if "Condition" in stmt:
                conditions = stmt["Condition"]
                print(f"     Conditions: {sum(len(v) if isinstance(v, dict) else 1 for v in conditions.values())} rules")
        
        # 4. Candidate Policy (LLM Generated)
        print(f"\n🧠 4. CANDIDATE POLICY (LLM Generated)")
        print("-" * 45)
        candidate_statements = artifacts.candidate_policy.get("Statement", [])
        print(f"📄 IAM Policy with {len(candidate_statements)} statements")
        print(f"📊 Generation Confidence: {artifacts.generation_confidence:.1%}")
        
        for i, stmt in enumerate(candidate_statements, 1):
            effect = stmt.get("Effect", "Unknown")
            actions = stmt.get("Action", [])
            if isinstance(actions, str):
                actions = [actions]
            print(f"  {i}. {stmt.get('Sid', f'Statement{i}')} ({effect})")
            print(f"     Actions: {len(actions)} ({', '.join(actions[:3])}{'...' if len(actions) > 3 else ''})")
            resources = stmt.get("Resource", [])
            if isinstance(resources, str):
                resources = [resources]
            print(f"     Resources: {len(resources)} ARNs")
            if "Condition" in stmt:
                conditions = stmt["Condition"]
                print(f"     Conditions: {sum(len(v) if isinstance(v, dict) else 1 for v in conditions.values())} rules")
        
        # 5. Comparison Summary
        print(f"\n📊 POLICY COMPARISON")
        print("-" * 25)
        baseline_stmt_count = len(baseline_statements)
        candidate_stmt_count = len(candidate_statements)
        print(f"Baseline statements: {baseline_stmt_count}")
        print(f"Candidate statements: {candidate_stmt_count}")
        
        if baseline_stmt_count == candidate_stmt_count:
            print("✅ Statement counts match")
        else:
            print(f"⚠️  Statement count difference: {abs(baseline_stmt_count - candidate_stmt_count)}")
        
        print(f"\n🎯 RECOMMENDATIONS")
        print("-" * 20)
        if artifacts.extraction_confidence < 0.8:
            print("• ⚠️  Low extraction confidence - review intent interpretation")
        if artifacts.generation_confidence < 0.8:
            print("• ⚠️  Low generation confidence - review LLM policy output")
        if baseline_stmt_count != candidate_stmt_count:
            print("• 🔍 Policy structure differs - compare baseline vs candidate carefully")
        if not artifacts.spec_dsl.must_never:
            print("• 🛡️  Consider adding explicit deny statements for security")
        
        print(f"\n✅ Four-artifact analysis complete!")
    
    def save_policy_artifacts(
        self, 
        artifacts: PolicyArtifacts, 
        rag_context: List[Dict[str, Any]], 
        original_prompt: str,
        custom_path: Optional[str] = None,
        output_dir: str = "outputs"
    ) -> Dict[str, str]:
        """
        Save all policy artifacts using the comprehensive ArtifactSaver.
        
        Returns:
            Dictionary with paths to all saved files
        """
        print(f"\n💾 Saving policy artifacts...")
        
        saver = ArtifactSaver(base_output_dir=output_dir)
        saved_files = saver.save_artifacts(
            artifacts=artifacts,
            original_prompt=original_prompt,
            rag_context=rag_context,
            custom_name=custom_path
        )
        
        print(f"✅ Artifacts saved successfully!")
        print(f"📁 Session directory: {Path(saved_files['master']).parent}")
        print(f"📄 Files created:")
        for file_type, file_path in saved_files.items():
            file_name = Path(file_path).name
            print(f"   • {file_type}: {file_name}")
        
        return saved_files

    def save_generated_policy(self, generated_policy: GeneratedPolicy, 
                             enhancement: PromptEnhancement, 
                             save_path: Optional[str] = None,
                             save_format: str = "both") -> List[str]:
        """
        Save the generated policy to file(s).
        
        Args:
            generated_policy: The policy to save
            enhancement: The original enhancement data
            save_path: Custom path (without extension), or auto-generate if None
            save_format: "json", "markdown", or "both"
            
        Returns:
            List of file paths that were created
        """
        if not generated_policy:
            print("❌ No policy to save")
            return []
        
        import datetime
        import os
        
        # Create policies directory if it doesn't exist
        policies_dir = "generated_policies"
        os.makedirs(policies_dir, exist_ok=True)
        
        # Generate filename if not provided
        if not save_path:
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            # Create safe filename from original prompt
            safe_prompt = "".join(c for c in enhancement.original_prompt if c.isalnum() or c in (' ', '-', '_')).rstrip()
            safe_prompt = "_".join(safe_prompt.split())[:50]  # Limit length
            save_path = f"{policies_dir}/{timestamp}_{safe_prompt}"
        
        saved_files = []
        
        # Save as JSON
        if save_format in ["json", "both"]:
            json_path = f"{save_path}.json"
            policy_data = {
                "metadata": {
                    "created_at": datetime.datetime.now().isoformat(),
                    "original_prompt": enhancement.original_prompt,
                    "enhanced_prompt": enhancement.enhanced_prompt,
                    "confidence_score": generated_policy.confidence_score
                },
                "policy": generated_policy.policy_json,
                "explanation": generated_policy.explanation,
                "security_notes": generated_policy.security_notes,
                "improvement_suggestions": generated_policy.improvement_suggestions
            }
            
            with open(json_path, 'w') as f:
                json.dump(policy_data, f, indent=2)
            saved_files.append(json_path)
            print(f"💾 Policy saved as JSON: {json_path}")
        
        # Save as Markdown
        if save_format in ["markdown", "both"]:
            md_path = f"{save_path}.md"
            
            with open(md_path, 'w') as f:
                f.write(f"# IAM Policy: {enhancement.original_prompt}\n\n")
                f.write(f"**Generated:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"**Confidence:** {generated_policy.confidence_score:.1%}\n\n")
                
                f.write("## Enhanced Prompt\n\n")
                f.write(f"{enhancement.enhanced_prompt}\n\n")
                
                f.write("## IAM Policy\n\n")
                f.write("```json\n")
                f.write(json.dumps(generated_policy.policy_json, indent=2))
                f.write("\n```\n\n")
                
                if generated_policy.explanation:
                    f.write("## Explanation\n\n")
                    f.write(f"{generated_policy.explanation}\n\n")
                
                if generated_policy.security_notes:
                    f.write("## Security Considerations\n\n")
                    for i, note in enumerate(generated_policy.security_notes, 1):
                        f.write(f"{i}. {note}\n")
                    f.write("\n")
                
                if generated_policy.improvement_suggestions:
                    f.write("## Improvement Suggestions\n\n")
                    for i, suggestion in enumerate(generated_policy.improvement_suggestions, 1):
                        f.write(f"{i}. {suggestion}\n")
            
            saved_files.append(md_path)
            print(f"📝 Policy saved as Markdown: {md_path}")
        
        return saved_files

    def _format_enhanced_prompt(self, enhanced_prompt: str) -> str:
        """Format the enhanced prompt for better readability - no longer needed since we show full text."""
        return enhanced_prompt

def main():
    parser = argparse.ArgumentParser(description="IAM Policy Research Agent with LLM Enhancement")
    parser.add_argument("prompt", nargs="?", help="User prompt for policy research")
    parser.add_argument("--enhance-only", help="Only enhance the prompt using vector search and LLM")
    parser.add_argument("--generate-policy", help="Generate complete IAM policy with LLM reasoning")
    parser.add_argument("--context", help="Additional context for policy generation")
    parser.add_argument("--api-base", default="http://localhost:8000", help="Vector DB API base URL")
    parser.add_argument("--openai-key", help="OpenAI API key (or set OPENAI_API_KEY env var)")
    parser.add_argument("--no-llm", action="store_true", help="Skip LLM enhancement (vector search only)")
    parser.add_argument("--save", help="Save generated policy to specified path (without extension)")
    parser.add_argument("--no-save", action="store_true", help="Don't auto-save generated policies")
    parser.add_argument("--save-format", choices=["json", "markdown", "both"], default="both", 
                       help="Format for saving policies (default: both)")
    parser.add_argument("--artifacts", action="store_true", 
                       help="NEW: Use four-artifact mode (ReadBack, SpecDSL, Baseline, Candidate)")
    parser.add_argument("--output-dir", default="outputs", 
                       help="Output directory for artifacts (default: outputs)")
    parser.add_argument("--session-name", 
                       help="Custom session name for artifact storage")
    
    args = parser.parse_args()
    
    # Initialize agent
    openai_key = args.openai_key if not args.no_llm else None
    agent = IAMPolicyAgent(api_base=args.api_base, openai_api_key=openai_key)
    
    if args.enhance_only:
        print("🔍 PROMPT ENHANCEMENT MODE")
        enhancement = agent.enhance_prompt_with_vector_context(args.enhance_only)
        
        print(f"\n📈 ENHANCEMENT RESULTS")
        print(f"Original: {enhancement.original_prompt}")
        print(f"Enhanced: {enhancement.enhanced_prompt}")
        if enhancement.llm_reasoning:
            print(f"\nLLM Reasoning: {enhancement.llm_reasoning}")
        print(f"Confidence: {enhancement.confidence_score:.1%}")
        if enhancement.missing_elements:
            print(f"Missing: {', '.join(enhancement.missing_elements)}")
        
    elif args.generate_policy:
        print("🏗️  COMPLETE POLICY GENERATION MODE")
        context_prompt = args.generate_policy
        if args.context:
            context_prompt += f" {args.context}"
        
        enhancement, context, generated_policy = agent.complete_policy_generation(context_prompt)
        
        # Display results
        agent.display_results(enhancement, context)
        if generated_policy:
            agent.display_generated_policy(generated_policy)
            
            # Auto-save by default when generating policy
            if not args.no_save:
                agent.save_generated_policy(generated_policy, enhancement, args.save, args.save_format)
        
    elif args.prompt:
        if args.artifacts:
            # NEW: Four-artifact mode
            print("🚀 NEW: FOUR-ARTIFACT MODE")
            artifacts, rag_context = agent.generate_policy_artifacts(args.prompt)
            agent.display_policy_artifacts(artifacts)
            
            # Save artifacts using the new comprehensive system
            if not args.no_save:
                saved_files = agent.save_policy_artifacts(
                    artifacts=artifacts,
                    rag_context=rag_context,
                    original_prompt=args.prompt,
                    custom_path=args.session_name,
                    output_dir=args.output_dir
                )
                print(f"\n🎯 To review results:")
                session_dir = Path(saved_files['master']).parent
                print(f"   cd {session_dir}")
                print(f"   cat README.md")
                
        elif not args.no_llm and agent.openai_client:
            print("🚀 FULL LLM-ENHANCED PIPELINE")
            enhancement, context, generated_policy = agent.complete_policy_generation(args.prompt)
            agent.display_results(enhancement, context)
            if generated_policy:
                agent.display_generated_policy(generated_policy)
                
                # Auto-save by default when LLM generates a policy
                if not args.no_save:
                    agent.save_generated_policy(generated_policy, enhancement, args.save, args.save_format)
        else:
            print("📚 VECTOR SEARCH RESEARCH MODE")
            enhancement, context = agent.research_policy_request(args.prompt)
            agent.display_results(enhancement, context)
        
    else:
        # Interactive demo
        print("🤖 IAM Policy Research Agent - Interactive Demo")
        print("Enhanced with LLM reasoning for intelligent policy generation")
        print("Enter prompts to see how the dual-index + LLM strategy works:")
        print("Type 'quit' to exit\n")
        
        while True:
            try:
                user_input = input("🔍 Enter your policy request: ").strip()
                if user_input.lower() in ['quit', 'exit', 'q']:
                    break
                if not user_input:
                    continue
                
                # Default to policy generation if LLM is available
                if agent.openai_client and not args.no_llm:
                    enhancement, context, generated_policy = agent.complete_policy_generation(user_input)
                    agent.display_results(enhancement, context)
                    if generated_policy:
                        agent.display_generated_policy(generated_policy)
                        
                        # Auto-save by default in interactive mode
                        if not args.no_save:
                            agent.save_generated_policy(generated_policy, enhancement)
                else:
                    enhancement, context = agent.research_policy_request(user_input)
                    agent.display_results(enhancement, context)
                
                print("\n" + "-" * 60 + "\n")
                
            except KeyboardInterrupt:
                print("\n\nGoodbye!")
                break
            except Exception as e:
                print(f"Error: {e}")

if __name__ == "__main__":
    main() 